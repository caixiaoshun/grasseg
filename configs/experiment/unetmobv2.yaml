# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - override /trainer: gpu
  - override /data: grass
  - override /model: unetmobv2
  - override /logger: wandb
  - override /callbacks: default

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["hrcWhu", "unetmobv2"]

seed: 42


  # scheduler:
  #   _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  #   _partial_: true
  #   mode: min
  #   factor: 0.1
  #   patience: 10

logger:
  wandb:
    project: "hrcwhu"
    name: "unetmobv2"
  aim:
    experiment: "hrcwhu_unetmobv2"

callbacks:
  model_checkpoint:
    dirpath: ${paths.output_dir}/checkpoints
    filename: "epoch_{epoch:03d}"
    monitor: "val/loss"
    mode: "min"
    save_last: True
    auto_insert_metric_name: False

  early_stopping:
    monitor: "val/loss"
    patience: 10
    mode: "min"